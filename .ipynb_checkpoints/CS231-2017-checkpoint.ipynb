{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1\n",
    "### 1. L1和L2区别 选择的原因？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2\n",
    "### 2. 训练集 测试集 验证集 的区别及选用?\n",
    "![](./image/cs231n/1-split-data.png)\n",
    "还有K折交叉验证 userful for small dataset but not well in DL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np的广播机制\n",
    "np.argsort()\n",
    "np.argmax()\n",
    "np.bincount()\n",
    "np.max()\n",
    "np.maxminum()\n",
    "np.random.binomial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss\n",
    "最大值为正无穷 最小值为负一。正样本分数变大 损失函数不变。\n",
    "比较接近时，损失函数是C-1。\n",
    "### 正则项\n",
    "使模型不是特复杂 噢卡姆剃刀原理。减轻模型复杂性 而不是为了拟合数据\n",
    "\n",
    "### L1和L2解释模型的复杂度是不一样的 。\n",
    "如[1,0,0,0]与[0,25,0.25,0.25,0.25] L1认为一样复杂 L2认为前者复杂。L1更喜欢稀疏解\n",
    "\n",
    "### softmax是为了使其更加接近概率分布\n",
    "- softmax最大值是\n",
    "### 区别\n",
    "- 合页损失是为了使正确的和错误大大于边界\n",
    "- softmax是为了better is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3 P9 optimizer\n",
    "- 梯度就是偏导函数组成的向量。梯度中每个元素可以告诉我们函数在相关方向上的斜率。所以负梯度就指向了下降最快的方向。梯度和该点单位方向向量的点积告诉了我们该点的斜率。\n",
    "- 有限差分计算。特别特别慢。\n",
    "- **梯度的负方向是函数下降最快的方向。** 方向导数的产物。梯度的定义就是函数上升最快的方向。\n",
    "[梯度完全理解](https://www.cnblogs.com/jackherrick/p/7118826.html)\n",
    "- 下一个问题：怎么求梯度??\n",
    "- 特征表示，比如颜色直方图等。类似于词袋。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P10 反向传播\n",
    "- 上游传回梯度乘本地梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P16权重初始化\n",
    "是一个大问题\n",
    "### P17 批量归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6318\n"
     ]
    }
   ],
   "source": [
    "###验证 np.random.choice的随机性如何\n",
    "import numpy as np\n",
    "a = set([])\n",
    "batch = 1000\n",
    "loop = 10\n",
    "for i in range(loop):\n",
    "    index = np.random.choice(loop*batch,batch)\n",
    "    a = a.union(set(index))\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment2 小结\n",
    "#### Loss\n",
    "- Hinge loss\n",
    " - 手撸。\n",
    "- Softmax loss\n",
    " - 手撸。\n",
    "#### 前向以及后向\n",
    "**其实 所有的后向传播都是计算本地梯度的操作。然后再进行chain rule。**\n",
    "\n",
    "- affine\n",
    " - 前向 OK\n",
    " - 后向 OK。后向中 偏置项只与输出类目有关。因为N组W对应N中输出可能，每种输出可能对应一个偏置项。反向的时候直接相加。**我的疑惑是为什么不求个均值呢？？因为是多出数据来更新的db啊**\n",
    "- bn\n",
    " - 前向 OK。就一个求均值 以及方差的公式\n",
    " - 后向\n",
    "- cnn\n",
    " - 前向 OK\n",
    " - 后向 \n",
    "- relu\n",
    " - 前向 OK\n",
    " - 后向。链式求导法则 等于上游梯度乘本地梯度。本地梯度为1 或者0(x<=0 时为0。由保存的cache 中前向x给出)\n",
    "- dropout \n",
    " - 前向 OK。mask = np.random.binomial(n=1, p=retain_prob, size=x.shape) out=x*mask\n",
    " - 后向\n",
    "- maxpool\n",
    " - 前向 OK\n",
    " - 后向\n",
    " \n",
    "- 为什么dout直接代表梯度了传回来了 就是(y_pred - y)？？\n",
    "\n",
    "#### 优化器\n",
    "- SGD\n",
    " - OK. x += (-lr * dx) \n",
    "- SGD+Momentum\n",
    " - OK. next_v = v + (-lr * dx) next_x = x + v\n",
    "- RMSP\n",
    "- Adam\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
